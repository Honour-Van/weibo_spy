# 基于Selenium模拟浏览器的微博爬虫

抽样爬取微博自2019年12月1日到2020年12月31日所有关于“武汉”的微博

从2019年12月到2020年12月，共约400天。每天分24小时，每小时收集50页*约20共1000条。这样的结果大约为600万条。

但是由于中途进程过慢，每个小时只爬取第一页。最终收集到约15万条数据。保存在weibo.zip中。

## 文件描述

- get_cookie.py：获取用户cookie
- date.py：生成所有正则日期，并存入date.json中
- spider.py：爬虫最终程序
- senti_recog.py：舆情分析程序，调用百度智能云API
- mytool.py：进度条工具程序

## 编写日志

基于notebook可以分步进行测试，一个cell可以相当于一个独立的操作，在进行开发的时候非常好用。

中途在5800左右开始失败，添加了一个异常处理的自启动机制，虽然并不是非常科学，但是在监督者离开之后，能够保证较好地连续作业。

其完成到8980次之后回退较多。随后我们决定重新启动。9049开始最终成功。